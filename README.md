<a name="top"></a>
# Bank Deposit Subscription Prediction â€“ Production-Ready ML Pipeline with FastAPI, Docker, and Render Deployment

This machine learning project predicts whether a client will subscribe to a bank deposit campaign using XGBoost, the best-performing model among all models trained. The project provides **Jupyter notebooks for EDA, preprocessing, and model training**, along with a **FastAPI-based REST API** deployed both locally via Docker **and in the cloud on Render**, enabling real-time subscription predictions.

---

## Table of Contents

- [Project Overview](#project-overview)  
- [Project Structure](#project-structure)  
- [Business Objective](#business-objective)  
- [Dataset](#dataset)  
- [Data Preparation](#data-preparation)  
- [Modeling Approach](#modeling-approach)  
- [Model Evaluation](#model-evaluation)  
- [Model Metrics Comparison](#model-metrics-comparison)  
- [Deployment Workflow](#deployment-workflow)  
- [Requirements](#requirements)  
- [Technologies](#technologies)  
- [Setup & Installation](#setup--installation)  
- [Running the API](#running-the-api)  
- [Testing the API](#testing-the-api)  
- [Docker](#docker)  
- [Render Deployment](#render-deployment)  
- [License](#license)

---

## Project Overview

Banks frequently run marketing campaigns to attract new deposits from clients. This project builds a **production-ready machine learning system** capable of:

- Predicting whether a client will subscribe to a term deposit campaign.  
- Producing probability-based subscription scores.  
- Deploying the best-performing model via FastAPI **locally and in the cloud using Render**.

The project covers the **full ML workflow** â€” from EDA and preprocessing to model selection, hyperparameter tuning, and deployment.

---

## Project Structure

```text
â”œâ”€â”€ Dockerfile                       # Dockerfile for containerizing the API (can be auto-generated)
â”œâ”€â”€ README.md                        # Project documentation
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ cleanup_repo.sh                   # Optional script to clean repository files
â”œâ”€â”€ generate_dockerfile.py            # Optional utility script to generate Dockerfile automatically
â”œâ”€â”€ data                              # Folder for raw and processed datasets
â”œâ”€â”€ figures/                          # Saved EDA plots and visualizations
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_EDA.ipynb                 # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_Preprocessing.ipynb       # Data preprocessing pipeline
â”‚   â”œâ”€â”€ 03_Baseline_Models.ipynb     # Baseline models (LogReg, etc.)
â”‚   â”œâ”€â”€ 04_Tree_Models.ipynb         # RandomForest & XGBoost
â”‚   â”œâ”€â”€ 05_Deep_and_Anomaly.ipynb    # MLP & anomaly detection models
â”‚   â”œâ”€â”€ 06_Model_Evaluation.ipynb    # Final model selection
â”œâ”€â”€ main.py                          # FastAPI application (can be generated automatically)
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ preprocessor.pkl              # Preprocessing pipeline
â”‚   â””â”€â”€ best_model.json               # Metadata for selected model
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model.pkl                # Final deployed model
â”œâ”€â”€ test_api.py                       # Local Test API predictions
â”œâ”€â”€ cloud_test_api.py                 # Cloud (Render) Test API predictions
â””â”€â”€ sample_requests/
    â”œâ”€â”€ client.json                   # Single client input
    â””â”€â”€ batch_clients.json            # Batch input
```
---

## Business Objective
### Problem Statement

Financial institutions need to identify which clients are likely to subscribe to a bank deposit to **improve campaign efficiency and reduce marketing costs**.

### Goal

Predict whether a client will subscribe to a bank deposit using machine learning.
- The model outputs a probability between 0 and 1 representing subscription likelihood.
- Default threshold = 0.5:
    - `proba >= 0.5` â†’ predicted subscriber
    - `proba < 0.5` â†’ predicted non-subscriber

âš ï¸ For imbalanced datasets (~11% subscribed), thresholds can be adjusted (e.g., 0.3) to increase recall.

### Impact
- Focus marketing efforts on clients likely to subscribe.
- Reduce cost of outreach.
- Improve campaign ROI.
---

## Dataset
### Source

Kaggle: [Bank Marketing Campaign Dataset](https://archive.ics.uci.edu/dataset/222/bank+marketing)

### Description
- 45,211 client records with demographic, economic, and campaign features.
- Target: y â†’ 1 = subscribed, 0 = not subscribed.
- Features include:
    - `age, job, marital, education`
    - `default, housing, loan`
    - `contact, month, day_of_week, duration`
    - `campaign, pdays, previous, poutcome`

### Why this dataset works
- Public and structured for supervised learning.
- Slight class imbalance (~11% subscribed), suitable for PR-AUC evaluation.
- Can apply various models: RandomForest, XGBoost, Logistic Regression, MLP.

---

## Data Preparation
### Exploratory Data Analysis (EDA)
- Examine class balance.
- Visualize categorical and numerical distributions.
- Inspect correlations between features and target.
### Preprocessing
- Encode categorical features (Label, OneHot, CatBoost).
- Scale numerical features.
- Handle class imbalance with SMOTE or class weights.
- Split dataset into train / validation / test (60/20/20).
- Save splits and preprocessor pipeline for reproducibility.

---

## Modeling Approach
### Models Trained
1. Logistic Regression (LR)
2. Support Vector Machine (SVM)
3. Random Forest (RF)
4. XGBoost
5. Multi-Layer Perceptron (MLP)
6. IsolationForest / One-Class SVM / Autoencoder (anomaly detection)
### Model Selection
- Evaluated all models using PR-AUC as primary metric.
    - Focuses on positive class performance.
    - Ideal when the dataset is imbalanced, i.e., the number of positive cases (subscribers/fraud) is much smaller than negative cases.
    - Measures how well the model balances precision and recall across thresholds.
    - Why itâ€™s ideal here: In the bank marketing dataset or fraud detection, only a small fraction subscribe, so PR AUC gives a more realistic view of performance than accuracy.

- Top 2 models tuned with Optuna.
- Final model automatically selected based on PR-AUC on test set.

### Deployment Choice
- XGBoost selected and deployed as production model.
---
## Model Evaluation

Metrics used (focus on subscription detection):
- PR-AUC (Precisionâ€“Recall AUC) â€“ Primary metric
- ROC-AUC â€“ Secondary metric
- F1 Score, Precision, Recall â€“ for interpretability
- Thresholds adjustable depending on campaign priorities


## ðŸ“Š Model Metrics Comparison

| Model              | roc_auc | pr_auc  | f1      | precision | recall  | threshold |
|------------------- |--------:|--------:|--------:|----------:|--------:|----------:|
| logreg             | 0.9235  | 0.5606  | 0.5931  | 0.4537    | 0.8562  | 0.5000    |
| random_forest      | 0.9310  | 0.6066 âœ… | 0.5746  | 0.5807    | 0.5687  | 0.5000    |
| xgboost            | 0.9334 âœ… | 0.6033  | 0.5844 âœ… | 0.5954 âœ… | 0.5738  | 0.5000    |
| mlp                | 0.9078  | 0.5308  | 0.3995  | 0.2503    | 0.9883 âœ… | 0.0000    |
| isolation_forest   | 0.5929  | 0.1883  | 0.2471  | 0.1549    | 0.6114  | -0.0447   |
| one_class_svm      | 0.5904  | 0.1746  | 0.2513  | 0.1575    | 0.6218  | -26.2506  |
| autoencoder        | 0.6289  | 0.2093  | 0.2686  | 0.1683    | 0.6645  | 0.0050    |

**Legend:**  
- âœ… Best value per metric  
- `roc_auc` = Area under the ROC curve  
- `pr_auc` = Area under the Precision-Recall curve  
- `f1` = F1-score  
- `precision`, `recall` = standard classification metrics  
- `threshold` = probability threshold used for prediction

### Final Best Model After Optuna Hyperparameter Tuning

#### Model: XGBoost

**Metrics on Test Data:**

| Metric      | Value  |
|------------ |-------:|
| roc_auc     | 0.9379 âœ… |
| pr_auc      | 0.6347 âœ… |
| f1          | 0.6317 âœ… |
| precision   | 0.5742  |
| recall      | 0.7021  |
| threshold   | 0.5000  |

**Notes:**  
- âœ… Indicates the best performance compared to previous models.  
- Threshold = probability cutoff used for binary classification.  
- Metrics were obtained on the held-out test dataset after hyperparameter tuning with Optuna.
